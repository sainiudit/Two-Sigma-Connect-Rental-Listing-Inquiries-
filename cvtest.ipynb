{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import operator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import xgboost as xgb\n",
    "import random\n",
    "from sklearn import model_selection, preprocessing, ensemble\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from textblob import TextBlob\n",
    "#input data\n",
    "train_df=pd.read_json('../input/train.json')\n",
    "test_df=pd.read_json('../input/test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/cluster/birch.py:602: UserWarning: Number of subclusters found (95) by Birch is less than (200). Decrease the threshold.\n",
      "  % (len(centroids), self.n_clusters))\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/cluster/birch.py:602: UserWarning: Number of subclusters found (97) by Birch is less than (200). Decrease the threshold.\n",
      "  % (len(centroids), self.n_clusters))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import Birch\n",
    "def cluster_latlon(n_clusters, data):  \n",
    "    #split the data between \"around NYC\" and \"other locations\" basically our first two clusters \n",
    "    data_c=data[(data.longitude>-74.05)&(data.longitude<-73.75)&(data.latitude>40.4)&(data.latitude<40.9)]\n",
    "    data_e=data[~(data.longitude>-74.05)&(data.longitude<-73.75)&(data.latitude>40.4)&(data.latitude<40.9)]\n",
    "    #put it in matrix form\n",
    "    coords=data_c.as_matrix(columns=['latitude', \"longitude\"])\n",
    "    \n",
    "    brc = Birch(branching_factor=100, n_clusters=n_clusters, threshold=0.01,compute_labels=True)\n",
    "\n",
    "    brc.fit(coords)\n",
    "    clusters=brc.predict(coords)\n",
    "    data_c[\"cluster_\"+str(n_clusters)]=clusters\n",
    "    data_e[\"cluster_\"+str(n_clusters)]=-1 #assign cluster label -1 for the non NYC listings \n",
    "    data=pd.concat([data_c,data_e])\n",
    "    #plt.scatter(data_c[\"longitude\"], data_c[\"latitude\"], c=data_c[\"cluster_\"+str(n_clusters)], s=10, linewidth=0.1)\n",
    "    #plt.title(str(n_clusters)+\" Neighbourhoods from clustering\")\n",
    "    #plt.show()\n",
    "    return data \n",
    "\n",
    "traingpsclusters=cluster_latlon(200, train_df[['listing_id','latitude','longitude']])\n",
    "traingpsclusters=traingpsclusters.drop(['latitude','longitude'],axis=1)\n",
    "\n",
    "testgpsclusters=cluster_latlon(200, test_df[['listing_id','latitude','longitude']])\n",
    "testgpsclusters=testgpsclusters.drop(['latitude','longitude'],axis=1)\n",
    "\n",
    "train_df=pd.merge(train_df,traingpsclusters,on='listing_id',how='left')\n",
    "test_df=pd.merge(test_df,testgpsclusters,on='listing_id',how='left')\n",
    "\n",
    "clusters_price_map=dict(train_df.groupby(by=\"cluster_200\")[\"price\"].median())\n",
    "train_df[\"price_comparison\"]=train_df['price']-train_df[\"cluster_200\"].map(clusters_price_map)\n",
    "\n",
    "clusters_price_map=dict(test_df.groupby(by=\"cluster_200\")[\"price\"].median())\n",
    "test_df[\"price_comparison\"]=test_df['price']-test_df[\"cluster_200\"].map(clusters_price_map)\n",
    "\n",
    "def create_binary_features(df):\n",
    "    bows = {\n",
    "        \"dogs\": (\"dogs\", \"dog\"),\n",
    "        \"cats\": (\"cats\",),\n",
    "        \"nofee\": (\"no fee\", \"no-fee\", \"no  fee\", \"nofee\", \"no_fee\"),\n",
    "        \"lowfee\": (\"reduced_fee\", \"low_fee\", \"reduced fee\", \"low fee\"),\n",
    "        \"furnished\": (\"furnished\",),\n",
    "        \"parquet\": (\"parquet\", \"hardwood\"),\n",
    "        \"concierge\": (\"concierge\", \"doorman\", \"housekeep\", \"in_super\"),\n",
    "        \"prewar\": (\"prewar\", \"pre_war\", \"pre war\", \"pre-war\"),\n",
    "        \"laundry\": (\"laundry\", \"lndry\"),\n",
    "        \"health\": (\"health\", \"gym\", \"fitness\", \"training\"),\n",
    "        \"transport\": (\"train\", \"subway\", \"transport\"),\n",
    "        \"parking\": (\"parking\",),\n",
    "        \"utilities\": (\"utilities\", \"heat water\", \"water included\")\n",
    "    }\n",
    "\n",
    "    def indicator(bow):\n",
    "        return lambda s: int(any([x in s for x in bow]))\n",
    "\n",
    "    features = df[\"features\"].apply(lambda f: \" \".join(f).lower())   # convert features to string\n",
    "    featurelist=[]\n",
    "    for key in bows:\n",
    "        df[\"feature_\" + key] = features.apply(indicator(bows[key]))\n",
    "        featurelist.append(\"feature_\" + key)\n",
    "    return df,featurelist\n",
    "\n",
    "train_df,featurelist=create_binary_features(train_df)\n",
    "test_df,featurelist=create_binary_features(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#basic features\n",
    "train_df[\"price_t\"] =train_df[\"price\"]/train_df[\"bedrooms\"]\n",
    "test_df[\"price_t\"] = test_df[\"price\"]/test_df[\"bedrooms\"] \n",
    "train_df[\"room_sum\"] = train_df[\"bedrooms\"]+train_df[\"bathrooms\"] \n",
    "test_df[\"room_sum\"] = test_df[\"bedrooms\"]+test_df[\"bathrooms\"] \n",
    "\n",
    "# count of photos #\n",
    "train_df[\"num_photos\"] = train_df[\"photos\"].apply(len)\n",
    "test_df[\"num_photos\"] = test_df[\"photos\"].apply(len)\n",
    "\n",
    "# count of \"features\" #\n",
    "train_df[\"num_features\"] = train_df[\"features\"].apply(len)\n",
    "test_df[\"num_features\"] = test_df[\"features\"].apply(len)\n",
    "\n",
    "# count of words present in description column #\n",
    "train_df[\"num_description_words\"] = train_df[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
    "test_df[\"num_description_words\"] = test_df[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
    "\n",
    "\n",
    "train_df['created'] = pd.to_datetime(train_df['created'])\n",
    "train_df['date'] = train_df['created'].dt.date\n",
    "train_df[\"year\"] = train_df[\"created\"].dt.year\n",
    "train_df['month'] = train_df['created'].dt.month\n",
    "train_df['day'] = train_df['created'].dt.day\n",
    "train_df['hour'] = train_df['created'].dt.hour\n",
    "train_df['weekday'] = train_df['created'].dt.weekday\n",
    "train_df['week'] = train_df['created'].dt.week\n",
    "train_df['quarter'] = train_df['created'].dt.quarter\n",
    "train_df['weekend'] = ((train_df['weekday'] == 5) & (train_df['weekday'] == 6))\n",
    "train_df['wd'] = ((train_df['weekday'] != 5) & (train_df['weekday'] != 6))\n",
    "\n",
    "test_df['created'] = pd.to_datetime(test_df['created'])\n",
    "test_df['date'] = test_df['created'].dt.date\n",
    "test_df[\"year\"] = test_df[\"created\"].dt.year\n",
    "test_df['month'] = test_df['created'].dt.month\n",
    "test_df['day'] = test_df['created'].dt.day\n",
    "test_df['hour'] = test_df['created'].dt.hour\n",
    "test_df['weekday'] = test_df['created'].dt.weekday\n",
    "test_df['week'] = test_df['created'].dt.week\n",
    "test_df['quarter'] = test_df['created'].dt.quarter\n",
    "test_df['weekend'] = ((test_df['weekday'] == 5) & (test_df['weekday'] == 6))\n",
    "test_df['wd'] = ((test_df['weekday'] != 5) & (test_df['weekday'] != 6))\n",
    "\n",
    "train_df = train_df.join(\n",
    "                   train_df['description'].apply(\n",
    "                       lambda x: TextBlob(x).sentiment.polarity).rename('sentiment'))\n",
    "\n",
    "test_df = test_df.join(\n",
    "                   test_df['description'].apply(\n",
    "                       lambda x: TextBlob(x).sentiment.polarity).rename('sentiment'))\n",
    "\n",
    "train_df[\"pos\"] = train_df.longitude.round(3).astype(str) + '_' + train_df.latitude.round(3).astype(str)\n",
    "\n",
    "vals = train_df['pos'].value_counts()\n",
    "dvals = vals.to_dict()\n",
    "train_df[\"density\"] = train_df['pos'].apply(lambda x: dvals.get(x, vals.min()))\n",
    "\n",
    "test_df[\"pos\"] = test_df.longitude.round(3).astype(str) + '_' + test_df.latitude.round(3).astype(str)\n",
    "\n",
    "vals = test_df['pos'].value_counts()\n",
    "dvals = vals.to_dict()\n",
    "test_df[\"density\"] = test_df['pos'].apply(lambda x: dvals.get(x, vals.min()))\n",
    "\n",
    "features_to_use=[\"price_comparison\",\"density\",\"sentiment\",\"wd\",\"weekend\",\"quarter\",\"week\",\"weekday\",\"hour\",\"day\",\"month\",\"year\",\"bathrooms\", \"bedrooms\", \"latitude\", \"longitude\", \"price\",\"price_t\",\"num_photos\", \"num_features\", \"num_description_words\",\"listing_id\"]\n",
    "features_to_use=features_to_use+featurelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgbm\n",
    "def runlgbm(train_X, train_y, test_X, test_y=None, feature_names=None, seed_val=0, num_rounds=500000,e_stoping_r=50): \n",
    "    t4_params = {\n",
    "        'boosting_type': 'gbdt', 'objective': 'multiclass', 'nthread': -1, 'silent': True,\n",
    "        'num_leaves': 6, 'learning_rate': 0.03, 'max_depth': 6,\n",
    "        'max_bin': 255, 'subsample_for_bin': 50000,\n",
    "        'subsample': 0.6, 'subsample_freq': 1, 'colsample_bytree': 0.6, 'reg_alpha':1, 'reg_lambda':0,\n",
    "        'min_split_gain': 0.5, 'min_child_weight': 1, 'min_child_samples': 10, 'scale_pos_weight': 1}\n",
    "\n",
    "    # they can be used directly to build a LGBMClassifier (which is wrapped in a sklearn fashion)\n",
    "    model = lgbm.sklearn.LGBMClassifier(n_estimators=num_rounds, seed=0, **t4_params)\n",
    "    \n",
    "    if test_y is not None:\n",
    "        model.fit(train_X,train_y,eval_set=[(train_X,train_y),(test_X, test_y)],verbose=100,early_stopping_rounds=e_stoping_r)\n",
    "    else:\n",
    "        model.fit(train_X,train_y)\n",
    "    pred_test_y = model.predict_proba(test_X)\n",
    "    return pred_test_y, model\n",
    "\n",
    "\n",
    "def runXGB(train_X, train_y, test_X, test_y=None, feature_names=None, seed_val=0, num_rounds=1000):\n",
    "    param = {}\n",
    "    param['objective'] = 'multi:softprob'\n",
    "    param['eta'] = 0.03\n",
    "    param['max_depth'] = 6\n",
    "    param['silent'] = 1\n",
    "    param['num_class'] = 3\n",
    "    param['eval_metric'] = \"mlogloss\"\n",
    "    param['min_child_weight'] = 1\n",
    "    param['subsample'] = 0.7\n",
    "    param['colsample_bytree'] = 0.7\n",
    "    param['seed'] = seed_val\n",
    "    num_rounds = num_rounds\n",
    "\n",
    "    plst = list(param.items())\n",
    "    xgtrain = xgb.DMatrix(train_X, label=train_y)\n",
    "\n",
    "    if test_y is not None:\n",
    "        xgtest = xgb.DMatrix(test_X, label=test_y)\n",
    "        watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n",
    "        model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=20)\n",
    "    else:\n",
    "        xgtest = xgb.DMatrix(test_X)\n",
    "        model = xgb.train(plst, xgtrain, num_rounds)\n",
    "\n",
    "    pred_test_y = model.predict(xgtest)\n",
    "    return pred_test_y, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index=list(range(train_df.shape[0]))\n",
    "random.shuffle(index)\n",
    "a=[np.nan]*len(train_df)\n",
    "b=[np.nan]*len(train_df)\n",
    "c=[np.nan]*len(train_df)\n",
    "\n",
    "for i in range(5):\n",
    "    building_level={}\n",
    "    for j in train_df['manager_id'].values:\n",
    "        building_level[j]=[0,0,0]\n",
    "    test_index=index[int((i*train_df.shape[0])/5):int(((i+1)*train_df.shape[0])/5)]\n",
    "    train_index=list(set(index).difference(test_index))\n",
    "    for j in train_index:\n",
    "        temp=train_df.iloc[j]\n",
    "        if temp['interest_level']=='low':\n",
    "            building_level[temp['manager_id']][0]+=1\n",
    "        if temp['interest_level']=='medium':\n",
    "            building_level[temp['manager_id']][1]+=1\n",
    "        if temp['interest_level']=='high':\n",
    "            building_level[temp['manager_id']][2]+=1\n",
    "    for j in test_index:\n",
    "        temp=train_df.iloc[j]\n",
    "        if sum(building_level[temp['manager_id']])!=0:\n",
    "            a[j]=building_level[temp['manager_id']][0]*1.0/sum(building_level[temp['manager_id']])\n",
    "            b[j]=building_level[temp['manager_id']][1]*1.0/sum(building_level[temp['manager_id']])\n",
    "            c[j]=building_level[temp['manager_id']][2]*1.0/sum(building_level[temp['manager_id']])\n",
    "train_df['manager_level_low']=a\n",
    "train_df['manager_level_medium']=b\n",
    "train_df['manager_level_high']=c\n",
    "\n",
    "\n",
    "\n",
    "a=[]\n",
    "b=[]\n",
    "c=[]\n",
    "building_level={}\n",
    "for j in train_df['manager_id'].values:\n",
    "    building_level[j]=[0,0,0]\n",
    "for j in range(train_df.shape[0]):\n",
    "    temp=train_df.iloc[j]\n",
    "    if temp['interest_level']=='low':\n",
    "        building_level[temp['manager_id']][0]+=1\n",
    "    if temp['interest_level']=='medium':\n",
    "        building_level[temp['manager_id']][1]+=1\n",
    "    if temp['interest_level']=='high':\n",
    "        building_level[temp['manager_id']][2]+=1\n",
    "\n",
    "for i in test_df['manager_id'].values:\n",
    "    if i not in building_level.keys():\n",
    "        a.append(np.nan)\n",
    "        b.append(np.nan)\n",
    "        c.append(np.nan)\n",
    "    else:\n",
    "        a.append(building_level[i][0]*1.0/sum(building_level[i]))\n",
    "        b.append(building_level[i][1]*1.0/sum(building_level[i]))\n",
    "        c.append(building_level[i][2]*1.0/sum(building_level[i]))\n",
    "test_df['manager_level_low']=a\n",
    "test_df['manager_level_medium']=b\n",
    "test_df['manager_level_high']=c\n",
    "\n",
    "features_to_use.append('manager_level_low') \n",
    "features_to_use.append('manager_level_medium') \n",
    "features_to_use.append('manager_level_high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categorical = [\"display_address\", \"manager_id\", \"building_id\", \"street_address\"]\n",
    "for f in categorical:\n",
    "        if train_df[f].dtype=='object':\n",
    "            #print(f)\n",
    "            lbl = preprocessing.LabelEncoder()\n",
    "            lbl.fit(list(train_df[f].values) + list(test_df[f].values))\n",
    "            train_df[f] = lbl.transform(list(train_df[f].values))\n",
    "            test_df[f] = lbl.transform(list(test_df[f].values))\n",
    "            features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def cart2rho(x, y):\n",
    "    rho = np.sqrt(x**2 + y**2)\n",
    "    return rho\n",
    "\n",
    "\n",
    "def cart2phi(x, y):\n",
    "    phi = np.arctan2(y, x)\n",
    "    return phi\n",
    "\n",
    "\n",
    "def rotation_x(row, alpha):\n",
    "    x = row['latitude']\n",
    "    y = row['longitude']\n",
    "    return x*math.cos(alpha) + y*math.sin(alpha)\n",
    "\n",
    "\n",
    "def rotation_y(row, alpha):\n",
    "    x = row['latitude']\n",
    "    y = row['longitude']\n",
    "    return y*math.cos(alpha) - x*math.sin(alpha)\n",
    "\n",
    "\n",
    "def add_rotation(degrees, df):\n",
    "    namex = \"rot\" + str(degrees) + \"_X\"\n",
    "    namey = \"rot\" + str(degrees) + \"_Y\"\n",
    "\n",
    "    df['num_' + namex] = df.apply(lambda row: rotation_x(row, math.pi/(180/degrees)), axis=1)\n",
    "    df['num_' + namey] = df.apply(lambda row: rotation_y(row, math.pi/(180/degrees)), axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def operate_on_coordinates(tr_df, te_df):\n",
    "    for df in [tr_df, te_df]:\n",
    "        #polar coordinates system\n",
    "        df[\"num_rho\"] = df.apply(lambda x: cart2rho(x[\"latitude\"] - 40.78222222, x[\"longitude\"]+73.96527777), axis=1)\n",
    "        df[\"num_phi\"] = df.apply(lambda x: cart2phi(x[\"latitude\"] - 40.78222222, x[\"longitude\"]+73.96527777), axis=1)\n",
    "        #rotations\n",
    "        for angle in [15,30,45,60]:\n",
    "            df = add_rotation(angle, df)\n",
    "\n",
    "    return tr_df, te_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allcols=train_df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df, test_df = operate_on_coordinates(train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/pandas/core/indexing.py:141: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def cap_share(x):\n",
    "    return sum(1 for c in x if c.isupper())/float(len(x)+1)\n",
    "\n",
    "for df in [train_df, test_df]:\n",
    "    # do you think that users might feel annoyed BY A DESCRIPTION THAT IS SHOUTING AT THEM?\n",
    "    df['num_cap_share'] = df['description'].apply(cap_share)\n",
    "    \n",
    "    # how long in lines the desc is?\n",
    "    df['num_nr_of_lines'] = df['description'].apply(lambda x: x.count('<br /><br />'))\n",
    "   \n",
    "    # is the description redacted by the website?        \n",
    "    df['num_redacted'] = 0\n",
    "    df['num_redacted'].ix[df['description'].str.contains('website_redacted')] = 1\n",
    "\n",
    "    \n",
    "    # can we contact someone via e-mail to ask for the details?\n",
    "    df['num_email'] = 0\n",
    "    df['num_email'].ix[df['description'].str.contains('@')] = 1\n",
    "    \n",
    "    #and... can we call them?\n",
    "    \n",
    "    reg = re.compile(\".*?(\\(?\\d{3}\\D{0,3}\\d{3}\\D{0,3}\\d{4}).*?\", re.S)\n",
    "    def try_and_find_nr(description):\n",
    "        if reg.match(description) is None:\n",
    "            return 0\n",
    "        return 1\n",
    "\n",
    "    df['num_phone_nr'] = df['description'].apply(try_and_find_nr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpdand_desnewfeaure=[col for col in train_df.columns if col not in allcols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_to_use.extend(gpdand_desnewfeaure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3481 unique managers\n",
    "temp = train_df.groupby('manager_id').count().iloc[:,-1]\n",
    "temp2 = test_df.groupby('manager_id').count().iloc[:,-1]\n",
    "train_managers = pd.concat([temp,temp2],axis=1,join='outer')\n",
    "train_managers.columns=['train_count','test_count']\n",
    "#print(train_managers.sort_values(by = 'train_count',ascending = False).head())\n",
    "# considering only those manager_ids which are in train\n",
    "man_list = train_managers['train_count'].sort_values(ascending = False).head(3481).index\n",
    "ixes = train_df.manager_id.isin(man_list)\n",
    "train10 = train_df[ixes][['manager_id','interest_level']]\n",
    "# create dummies of interest levels\n",
    "interest_dummies = pd.get_dummies(train10.interest_level)\n",
    "train10 = pd.concat([train10,interest_dummies[['low','medium','high']]], axis = 1).drop('interest_level', axis = 1)\n",
    "#print(train10.head())\n",
    "gby = pd.concat([train10.groupby('manager_id').mean(),train10.groupby('manager_id').count()], axis = 1).iloc[:,:-2]\n",
    "gby.columns = ['low','medium','high','count']\n",
    "gby.sort_values(by = 'count', ascending = False).head(10)\n",
    "gby['manager_skill'] = gby['medium']*1 + gby['high']*2 \n",
    "gby['manager_id']=gby.index\n",
    "#print(gby.head(5))\n",
    "#print(gby.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.merge(train_df,gby[['manager_id','manager_skill']],on='manager_id',how='left')\n",
    "train_df['manager_skill']=train_df['manager_skill'].fillna(0)\n",
    "test_df = pd.merge(test_df,gby[['manager_id','manager_skill']],on='manager_id',how='left')\n",
    "test_df['manager_skill']=test_df['manager_skill'].fillna(0)\n",
    "\n",
    "features_to_use.append('manager_skill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df[\"features\"]=train_df[\"features\"].fillna(\"empty\")\n",
    "test_df[\"features\"]=test_df[\"features\"].fillna(\"empty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df['features'] = train_df[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))\n",
    "test_df['features'] = test_df[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))\n",
    "\n",
    "tfidf = CountVectorizer(stop_words='english', max_features=200)\n",
    "tr_sparse = tfidf.fit_transform(train_df[\"features\"])\n",
    "te_sparse = tfidf.transform(test_df[\"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df['wd']=train_df['wd'].astype(int)\n",
    "train_df['weekend']=train_df['weekend'].astype(int)\n",
    "\n",
    "test_df['wd']=test_df['wd'].astype(int)\n",
    "test_df['weekend']=test_df['weekend'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((49352, 258), (74659, 258))\n"
     ]
    }
   ],
   "source": [
    "train_X = sparse.hstack([train_df[features_to_use], tr_sparse]).tocsr()\n",
    "test_X = sparse.hstack([test_df[features_to_use], te_sparse]).tocsr()\n",
    "\n",
    "target_num_map = {'high':0, 'medium':1, 'low':2}\n",
    "train_y = np.array(train_df['interest_level'].apply(lambda x: target_num_map[x]))\n",
    "print(train_X.shape, test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cv_scores = []\n",
    "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2016)\n",
    "for dev_index, val_index in kf.split(range(train_X.shape[0])):\n",
    "        dev_X, val_X = train_X[dev_index,:], train_X[val_index,:]\n",
    "        dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "        preds, model = runlgbm(dev_X, dev_y, val_X, val_y)\n",
    "        cv_scores.append(log_loss(val_y, preds))\n",
    "        print(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#preds, model = runlgbm(train_X, train_y)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_X, train_y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train until valid scores didn't improve in 50 rounds.\n",
      "[100]\tvalid_0's multi_logloss: 0.589483\tvalid_1's multi_logloss: 0.599566\n",
      "[200]\tvalid_0's multi_logloss: 0.542201\tvalid_1's multi_logloss: 0.555292\n",
      "[300]\tvalid_0's multi_logloss: 0.522523\tvalid_1's multi_logloss: 0.538121\n",
      "[400]\tvalid_0's multi_logloss: 0.509164\tvalid_1's multi_logloss: 0.527624\n",
      "[500]\tvalid_0's multi_logloss: 0.498251\tvalid_1's multi_logloss: 0.519453\n",
      "[600]\tvalid_0's multi_logloss: 0.489303\tvalid_1's multi_logloss: 0.512841\n",
      "[700]\tvalid_0's multi_logloss: 0.481457\tvalid_1's multi_logloss: 0.507615\n",
      "[800]\tvalid_0's multi_logloss: 0.474562\tvalid_1's multi_logloss: 0.503475\n",
      "[900]\tvalid_0's multi_logloss: 0.468445\tvalid_1's multi_logloss: 0.500001\n",
      "[1000]\tvalid_0's multi_logloss: 0.462827\tvalid_1's multi_logloss: 0.496819\n",
      "[1100]\tvalid_0's multi_logloss: 0.457656\tvalid_1's multi_logloss: 0.493888\n",
      "[1200]\tvalid_0's multi_logloss: 0.452985\tvalid_1's multi_logloss: 0.491654\n",
      "[1300]\tvalid_0's multi_logloss: 0.448495\tvalid_1's multi_logloss: 0.489626\n",
      "[1400]\tvalid_0's multi_logloss: 0.44441\tvalid_1's multi_logloss: 0.487954\n",
      "[1500]\tvalid_0's multi_logloss: 0.440399\tvalid_1's multi_logloss: 0.486466\n",
      "[1600]\tvalid_0's multi_logloss: 0.436683\tvalid_1's multi_logloss: 0.484941\n",
      "[1700]\tvalid_0's multi_logloss: 0.433065\tvalid_1's multi_logloss: 0.483744\n",
      "[1800]\tvalid_0's multi_logloss: 0.429607\tvalid_1's multi_logloss: 0.482607\n",
      "[1900]\tvalid_0's multi_logloss: 0.426244\tvalid_1's multi_logloss: 0.48154\n",
      "[2000]\tvalid_0's multi_logloss: 0.422964\tvalid_1's multi_logloss: 0.480534\n",
      "[2100]\tvalid_0's multi_logloss: 0.419815\tvalid_1's multi_logloss: 0.479732\n",
      "[2200]\tvalid_0's multi_logloss: 0.416816\tvalid_1's multi_logloss: 0.479269\n",
      "[2300]\tvalid_0's multi_logloss: 0.413923\tvalid_1's multi_logloss: 0.478594\n",
      "[2400]\tvalid_0's multi_logloss: 0.411133\tvalid_1's multi_logloss: 0.478093\n",
      "[2500]\tvalid_0's multi_logloss: 0.408423\tvalid_1's multi_logloss: 0.477489\n",
      "[2600]\tvalid_0's multi_logloss: 0.405722\tvalid_1's multi_logloss: 0.476918\n",
      "[2700]\tvalid_0's multi_logloss: 0.403092\tvalid_1's multi_logloss: 0.476423\n",
      "[2800]\tvalid_0's multi_logloss: 0.4005\tvalid_1's multi_logloss: 0.475971\n",
      "[2900]\tvalid_0's multi_logloss: 0.398002\tvalid_1's multi_logloss: 0.475562\n",
      "[3000]\tvalid_0's multi_logloss: 0.39549\tvalid_1's multi_logloss: 0.475334\n",
      "[3100]\tvalid_0's multi_logloss: 0.393046\tvalid_1's multi_logloss: 0.474887\n",
      "[3200]\tvalid_0's multi_logloss: 0.39065\tvalid_1's multi_logloss: 0.474476\n",
      "[3300]\tvalid_0's multi_logloss: 0.388303\tvalid_1's multi_logloss: 0.474221\n",
      "[3400]\tvalid_0's multi_logloss: 0.386054\tvalid_1's multi_logloss: 0.474\n",
      "[3500]\tvalid_0's multi_logloss: 0.383843\tvalid_1's multi_logloss: 0.473686\n",
      "[3600]\tvalid_0's multi_logloss: 0.381646\tvalid_1's multi_logloss: 0.473431\n",
      "[3700]\tvalid_0's multi_logloss: 0.379535\tvalid_1's multi_logloss: 0.473246\n",
      "[3800]\tvalid_0's multi_logloss: 0.377428\tvalid_1's multi_logloss: 0.473088\n",
      "[3900]\tvalid_0's multi_logloss: 0.375384\tvalid_1's multi_logloss: 0.472973\n",
      "[4000]\tvalid_0's multi_logloss: 0.373341\tvalid_1's multi_logloss: 0.472774\n",
      "[4100]\tvalid_0's multi_logloss: 0.371325\tvalid_1's multi_logloss: 0.472352\n",
      "Early stopping, best iteration is:\n",
      "[4095]\tvalid_0's multi_logloss: 0.371426\tvalid_1's multi_logloss: 0.472335\n"
     ]
    }
   ],
   "source": [
    "preds, model = runlgbm(X_train, y_train,X_test,y_test,num_rounds=500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds, model = runlgbm(train_X, train_y,test_X,num_rounds=4095)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#preds=model.predict_proba(test_X)\n",
    "out_df = pd.DataFrame(preds)\n",
    "out_df.columns = [\"high\", \"medium\", \"low\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_df[\"listing_id\"] = test_df.listing_id.values\n",
    "out_df.to_csv(\"xgb_starter441.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_df = pd.DataFrame(preds)\n",
    "out_df.columns = [\"score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_num_map_inv = {0:'high', 1:'medium', 2:'low'}\n",
    "out_df['score']= out_df['score'].apply(lambda x: target_num_map_inv[x])\n",
    "dummy=pd.get_dummies(out_df['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_df=pd.concat([out_df,dummy],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_df[\"listing_id\"] = test_df.listing_id.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_df[[\"listing_id\",\"high\",\"medium\",\"low\"]].to_csv(\"submission_dummy.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
