{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import xgboost as xgb\n",
    "import random\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "train_df = pd.read_json(\"../input/train.json\")\n",
    "test_df = pd.read_json(\"../input/test.json\")\n",
    "\n",
    "def runXGB(train_X, train_y, test_X, test_y=None, feature_names=None, seed_val=321, num_rounds=2000):\n",
    "    param = {}\n",
    "    param['objective'] = 'multi:softprob'\n",
    "    param['eta'] = 0.02\n",
    "    param['max_depth'] = 6\n",
    "    param['silent'] = 1\n",
    "    param['num_class'] = 3\n",
    "    param['eval_metric'] = \"mlogloss\"\n",
    "    param['min_child_weight'] = 1\n",
    "    param['subsample'] = 0.7\n",
    "    param['colsample_bytree'] = 0.7\n",
    "    param['seed'] = seed_val\n",
    "    num_rounds = num_rounds\n",
    "\n",
    "    plst = list(param.items())\n",
    "    xgtrain = xgb.DMatrix(train_X, label=train_y)\n",
    "\n",
    "    if test_y is not None:\n",
    "        xgtest = xgb.DMatrix(test_X, label=test_y)\n",
    "        watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n",
    "        model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=20,verbose_eval=50)\n",
    "    else:\n",
    "        xgtest = xgb.DMatrix(test_X)\n",
    "        model = xgb.train(plst, xgtrain, num_rounds)\n",
    "\n",
    "    pred_test_y = model.predict(xgtest)\n",
    "    return pred_test_y, model\n",
    "\n",
    "import lightgbm as lgbm\n",
    "def runlgbm(train_X, train_y, test_X, test_y=None, feature_names=None, seed_val=0, num_rounds=500000,e_stoping_r=50): \n",
    "    t4_params = {\n",
    "        'boosting_type': 'gbdt', 'objective': 'multiclass', 'nthread': -1, 'silent': True,\n",
    "        'num_leaves': 6, 'learning_rate': 0.03, 'max_depth': 6,\n",
    "        'max_bin': 255, 'subsample_for_bin': 50000,\n",
    "        'subsample': 0.6, 'subsample_freq': 1, 'colsample_bytree': 0.6, 'reg_alpha':1, 'reg_lambda':0,\n",
    "        'min_split_gain': 0.5, 'min_child_weight': 1, 'min_child_samples': 10, 'scale_pos_weight': 1}\n",
    "\n",
    "    # they can be used directly to build a LGBMClassifier (which is wrapped in a sklearn fashion)\n",
    "    model = lgbm.sklearn.LGBMClassifier(n_estimators=num_rounds, seed=0, **t4_params)\n",
    "    \n",
    "    if test_y is not None:\n",
    "        model.fit(train_X,train_y,eval_set=[(train_X,train_y),(test_X, test_y)],verbose=100,early_stopping_rounds=e_stoping_r)\n",
    "    else:\n",
    "        model.fit(train_X,train_y)\n",
    "    pred_test_y = model.predict_proba(test_X)\n",
    "    return pred_test_y, model\n",
    "\n",
    "\n",
    "test_df[\"bathrooms\"].loc[19671] = 1.5\n",
    "test_df[\"bathrooms\"].loc[22977] = 2.0\n",
    "test_df[\"bathrooms\"].loc[63719] = 2.0\n",
    "train_df[\"price\"] = train_df[\"price\"].clip(upper=13000)\n",
    "\n",
    "train_df[\"logprice\"] = np.log(train_df[\"price\"])\n",
    "test_df[\"logprice\"] = np.log(test_df[\"price\"])\n",
    "\n",
    "train_df[\"price_t\"] =train_df[\"price\"]/train_df[\"bedrooms\"]\n",
    "test_df[\"price_t\"] = test_df[\"price\"]/test_df[\"bedrooms\"] \n",
    "\n",
    "train_df[\"room_sum\"] = train_df[\"bedrooms\"]+train_df[\"bathrooms\"] \n",
    "test_df[\"room_sum\"] = test_df[\"bedrooms\"]+test_df[\"bathrooms\"] \n",
    "\n",
    "train_df['price_per_room'] = train_df['price']/train_df['room_sum']\n",
    "test_df['price_per_room'] = test_df['price']/test_df['room_sum']\n",
    "\n",
    "train_df[\"num_photos\"] = train_df[\"photos\"].apply(len)\n",
    "test_df[\"num_photos\"] = test_df[\"photos\"].apply(len)\n",
    "\n",
    "train_df[\"num_features\"] = train_df[\"features\"].apply(len)\n",
    "test_df[\"num_features\"] = test_df[\"features\"].apply(len)\n",
    "\n",
    "train_df[\"num_description_words\"] = train_df[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
    "test_df[\"num_description_words\"] = test_df[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
    "\n",
    "train_df[\"created\"] = pd.to_datetime(train_df[\"created\"])\n",
    "test_df[\"created\"] = pd.to_datetime(test_df[\"created\"])\n",
    "train_df[\"created_year\"] = train_df[\"created\"].dt.year\n",
    "test_df[\"created_year\"] = test_df[\"created\"].dt.year\n",
    "train_df[\"created_month\"] = train_df[\"created\"].dt.month\n",
    "test_df[\"created_month\"] = test_df[\"created\"].dt.month\n",
    "train_df[\"created_day\"] = train_df[\"created\"].dt.day\n",
    "test_df[\"created_day\"] = test_df[\"created\"].dt.day\n",
    "train_df[\"created_hour\"] = train_df[\"created\"].dt.hour\n",
    "test_df[\"created_hour\"] = test_df[\"created\"].dt.hour\n",
    "\n",
    "\n",
    "train_df[\"pos\"] = train_df.longitude.round(3).astype(str) + '_' + train_df.latitude.round(3).astype(str)\n",
    "test_df[\"pos\"] = test_df.longitude.round(3).astype(str) + '_' + test_df.latitude.round(3).astype(str)\n",
    "\n",
    "vals = train_df['pos'].value_counts()\n",
    "dvals = vals.to_dict()\n",
    "train_df[\"density\"] = train_df['pos'].apply(lambda x: dvals.get(x, vals.min()))\n",
    "test_df[\"density\"] = test_df['pos'].apply(lambda x: dvals.get(x, vals.min()))\n",
    "\n",
    "features_to_use=[\"bathrooms\", \"bedrooms\", \"latitude\", \"longitude\", \"price\",\"price_t\",\"price_per_room\", \"logprice\", \"density\",\n",
    "\"num_photos\", \"num_features\", \"num_description_words\",\"listing_id\", \"created_year\", \"created_month\", \"created_day\", \"created_hour\"]\n",
    "\n",
    "index=list(range(train_df.shape[0]))\n",
    "random.shuffle(index)\n",
    "a=[np.nan]*len(train_df)\n",
    "b=[np.nan]*len(train_df)\n",
    "c=[np.nan]*len(train_df)\n",
    "\n",
    "for i in range(5):\n",
    "    building_level={}\n",
    "    for j in train_df['manager_id'].values:\n",
    "        building_level[j]=[0,0,0]\n",
    "    \n",
    "    test_index=index[int((i*train_df.shape[0])/5):int(((i+1)*train_df.shape[0])/5)]\n",
    "    train_index=list(set(index).difference(test_index))\n",
    "    \n",
    "    for j in train_index:\n",
    "        temp=train_df.iloc[j]\n",
    "        if temp['interest_level']=='low':\n",
    "            building_level[temp['manager_id']][0]+=1\n",
    "        if temp['interest_level']=='medium':\n",
    "            building_level[temp['manager_id']][1]+=1\n",
    "        if temp['interest_level']=='high':\n",
    "            building_level[temp['manager_id']][2]+=1\n",
    "            \n",
    "    for j in test_index:\n",
    "        temp=train_df.iloc[j]\n",
    "        if sum(building_level[temp['manager_id']])!=0:\n",
    "            a[j]=building_level[temp['manager_id']][0]*1.0/sum(building_level[temp['manager_id']])\n",
    "            b[j]=building_level[temp['manager_id']][1]*1.0/sum(building_level[temp['manager_id']])\n",
    "            c[j]=building_level[temp['manager_id']][2]*1.0/sum(building_level[temp['manager_id']])\n",
    "            \n",
    "train_df['manager_level_low']=a\n",
    "train_df['manager_level_medium']=b\n",
    "train_df['manager_level_high']=c\n",
    "\n",
    "a=[]\n",
    "b=[]\n",
    "c=[]\n",
    "building_level={}\n",
    "for j in train_df['manager_id'].values:\n",
    "    building_level[j]=[0,0,0]\n",
    "\n",
    "for j in range(train_df.shape[0]):\n",
    "    temp=train_df.iloc[j]\n",
    "    if temp['interest_level']=='low':\n",
    "        building_level[temp['manager_id']][0]+=1\n",
    "    if temp['interest_level']=='medium':\n",
    "        building_level[temp['manager_id']][1]+=1\n",
    "    if temp['interest_level']=='high':\n",
    "        building_level[temp['manager_id']][2]+=1\n",
    "\n",
    "for i in test_df['manager_id'].values:\n",
    "    if i not in building_level.keys():\n",
    "        a.append(np.nan)\n",
    "        b.append(np.nan)\n",
    "        c.append(np.nan)\n",
    "    else:\n",
    "        a.append(building_level[i][0]*1.0/sum(building_level[i]))\n",
    "        b.append(building_level[i][1]*1.0/sum(building_level[i]))\n",
    "        c.append(building_level[i][2]*1.0/sum(building_level[i]))\n",
    "test_df['manager_level_low']=a\n",
    "test_df['manager_level_medium']=b\n",
    "test_df['manager_level_high']=c\n",
    "\n",
    "features_to_use.append('manager_level_low') \n",
    "features_to_use.append('manager_level_medium') \n",
    "features_to_use.append('manager_level_high')\n",
    "\n",
    "categorical = [\"display_address\", \"manager_id\", \"building_id\"]\n",
    "for f in categorical:\n",
    "        if train_df[f].dtype=='object':\n",
    "            lbl = LabelEncoder()\n",
    "            lbl.fit(list(train_df[f].values) + list(test_df[f].values))\n",
    "            train_df[f] = lbl.transform(list(train_df[f].values))\n",
    "            test_df[f] = lbl.transform(list(test_df[f].values))\n",
    "            features_to_use.append(f)\n",
    "\n",
    "train_df['features'] = train_df[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))\n",
    "test_df['features'] = test_df[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))\n",
    "\n",
    "tfidf = CountVectorizer(stop_words='english', max_features=200)\n",
    "tr_sparse = tfidf.fit_transform(train_df[\"features\"])\n",
    "te_sparse = tfidf.transform(test_df[\"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/pandas/core/indexing.py:140: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "def cart2rho(x, y):\n",
    "    rho = np.sqrt(x**2 + y**2)\n",
    "    return rho\n",
    "\n",
    "\n",
    "def cart2phi(x, y):\n",
    "    phi = np.arctan2(y, x)\n",
    "    return phi\n",
    "\n",
    "\n",
    "def rotation_x(row, alpha):\n",
    "    x = row['latitude']\n",
    "    y = row['longitude']\n",
    "    return x*math.cos(alpha) + y*math.sin(alpha)\n",
    "\n",
    "\n",
    "def rotation_y(row, alpha):\n",
    "    x = row['latitude']\n",
    "    y = row['longitude']\n",
    "    return y*math.cos(alpha) - x*math.sin(alpha)\n",
    "\n",
    "\n",
    "def add_rotation(degrees, df):\n",
    "    namex = \"rot\" + str(degrees) + \"_X\"\n",
    "    namey = \"rot\" + str(degrees) + \"_Y\"\n",
    "\n",
    "    df['num_' + namex] = df.apply(lambda row: rotation_x(row, math.pi/(180/degrees)), axis=1)\n",
    "    df['num_' + namey] = df.apply(lambda row: rotation_y(row, math.pi/(180/degrees)), axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def operate_on_coordinates(tr_df, te_df):\n",
    "    for df in [tr_df, te_df]:\n",
    "        #polar coordinates system\n",
    "        df[\"num_rho\"] = df.apply(lambda x: cart2rho(x[\"latitude\"] - 40.78222222, x[\"longitude\"]+73.96527777), axis=1)\n",
    "        df[\"num_phi\"] = df.apply(lambda x: cart2phi(x[\"latitude\"] - 40.78222222, x[\"longitude\"]+73.96527777), axis=1)\n",
    "        #rotations\n",
    "        for angle in [15,30,45,60]:\n",
    "            df = add_rotation(angle, df)\n",
    "\n",
    "    return tr_df, te_df\n",
    "\n",
    "allcols=train_df.columns.tolist()\n",
    "train_df, test_df = operate_on_coordinates(train_df, test_df)\n",
    "\n",
    "import re\n",
    "\n",
    "def cap_share(x):\n",
    "    return sum(1 for c in x if c.isupper())/float(len(x)+1)\n",
    "\n",
    "for df in [train_df, test_df]:\n",
    "    # do you think that users might feel annoyed BY A DESCRIPTION THAT IS SHOUTING AT THEM?\n",
    "    df['num_cap_share'] = df['description'].apply(cap_share)\n",
    "    \n",
    "    # how long in lines the desc is?\n",
    "    df['num_nr_of_lines'] = df['description'].apply(lambda x: x.count('<br /><br />'))\n",
    "   \n",
    "    # is the description redacted by the website?        \n",
    "    df['num_redacted'] = 0\n",
    "    df['num_redacted'].ix[df['description'].str.contains('website_redacted')] = 1\n",
    "\n",
    "    \n",
    "    # can we contact someone via e-mail to ask for the details?\n",
    "    df['num_email'] = 0\n",
    "    df['num_email'].ix[df['description'].str.contains('@')] = 1\n",
    "    \n",
    "    #and... can we call them?\n",
    "    \n",
    "    reg = re.compile(\".*?(\\(?\\d{3}\\D{0,3}\\d{3}\\D{0,3}\\d{4}).*?\", re.S)\n",
    "    def try_and_find_nr(description):\n",
    "        if reg.match(description) is None:\n",
    "            return 0\n",
    "        return 1\n",
    "\n",
    "    df['num_phone_nr'] = df['description'].apply(try_and_find_nr)\n",
    "\n",
    "gpdand_desnewfeaure=[col for col in train_df.columns if col not in allcols]\n",
    "features_to_use.extend(gpdand_desnewfeaure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import s2sphere\n",
    "lat = train_df.latitude.values.tolist()\n",
    "lon = train_df.longitude.values.tolist()\n",
    "\n",
    "cellId1 = []\n",
    "cellId2 = []\n",
    "cellId3 = []\n",
    "for i in range(0,len(lat)):\n",
    "    p1 = s2sphere.LatLng.from_degrees(lat[i], lon[i])\n",
    "    cell = s2sphere.CellId.from_lat_lng(p1)\n",
    "    cid = str(cell.id())\n",
    "    #print(cid)\n",
    "    ##cid is a 19 digit number so python storing it as Object, not number\n",
    "    ##So I am converting it into 3 numbers\n",
    "    cellId1.append(int(cid[:6]))\n",
    "    cellId2.append(int(cid[6:12]))\n",
    "    cellId3.append(int(cid[12:19]))\n",
    "    \n",
    "\n",
    "se = pd.Series(cellId1)\n",
    "train_df['cellId1'] = se.values\n",
    "\n",
    "se = pd.Series(cellId2)\n",
    "train_df['cellId2'] = se.values\n",
    "\n",
    "se = pd.Series(cellId3)\n",
    "train_df['cellId3'] = se.values\n",
    "\n",
    "lat = test_df.latitude.values.tolist()\n",
    "lon = test_df.longitude.values.tolist()\n",
    "\n",
    "cellId1 = []\n",
    "cellId2 = []\n",
    "cellId3 = []\n",
    "for i in range(0,len(lat)):\n",
    "    p1 = s2sphere.LatLng.from_degrees(lat[i], lon[i])\n",
    "    cell = s2sphere.CellId.from_lat_lng(p1)\n",
    "    cid = str(cell.id())\n",
    "    #print(cid)\n",
    "    cellId1.append(int(cid[:6]))\n",
    "    cellId2.append(int(cid[6:12]))\n",
    "    cellId3.append(int(cid[12:19]))\n",
    "    \n",
    "\n",
    "se = pd.Series(cellId1)\n",
    "test_df['cellId1'] = se.values\n",
    "\n",
    "se = pd.Series(cellId2)\n",
    "test_df['cellId2'] = se.values\n",
    "\n",
    "se = pd.Series(cellId1)\n",
    "test_df['cellId3'] = se.values\n",
    "features_to_use.extend(['cellId1','cellId2','cellId3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "train_df['Des_TextBlob']=train_df.description.apply(lambda x: TextBlob(x))\n",
    "test_df['Des_TextBlob']=test_df.description.apply(lambda x: TextBlob(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df['des_sentiment']=train_df['Des_TextBlob'].apply(lambda x: x.sentiment.polarity)\n",
    "test_df['des_sentiment']=test_df['Des_TextBlob'].apply(lambda x: x.sentiment.polarity)\n",
    "\n",
    "train_df['des_subjectivity']=train_df['Des_TextBlob'].apply(lambda x: x.sentiment.polarity)\n",
    "test_df['des_subjectivity']=test_df['Des_TextBlob'].apply(lambda x: x.sentiment.polarity)\n",
    "\n",
    "features_to_use.extend(['des_sentiment','des_subjectivity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df['noun_phrases_count']=train_df.Des_TextBlob.apply(lambda x: len(x.noun_phrases))\n",
    "test_df['noun_phrases_count']=test_df.Des_TextBlob.apply(lambda x: len(x.noun_phrases))\n",
    "features_to_use.append('noun_phrases_count') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train_df['num_IMPORTANT']=0\n",
    "train_df['num_NOTICE']=0\n",
    "#test_df['num_IMPORTANT']=0\n",
    "test_df['num_NOTICE']=0\n",
    "\n",
    "#train_df['num_IMPORTANT'].ix[train_df['description'].str.contains('IMPORTANT')] = 1\n",
    "test_df['num_NOTICE'].ix[test_df['description'].str.contains('NOTICE')] = 1\n",
    "\n",
    "#train_df['num_IMPORTANT'].ix[train_df['description'].str.contains('IMPORTANT')] = 1\n",
    "test_df['num_NOTICE'].ix[test_df['description'].str.contains('NOTICE')] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_to_use.extend(['num_NOTICE']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#magic features\n",
    "image_date = pd.read_csv(\"../input/listing_image_time.csv\")\n",
    "\n",
    "# rename columns so you can join tables later on\n",
    "image_date.columns = [\"listing_id\", \"time_stamp\"]\n",
    "\n",
    "# reassign the only one timestamp from April, all others from Oct/Nov\n",
    "image_date.loc[80240,\"time_stamp\"] = 1478129766 \n",
    "\n",
    "image_date[\"img_date\"]                  = pd.to_datetime(image_date[\"time_stamp\"], unit=\"s\")\n",
    "image_date[\"img_days_passed\"]           = (image_date[\"img_date\"].max() - image_date[\"img_date\"]).astype(\"timedelta64[D]\").astype(int)\n",
    "image_date[\"img_date_month\"]            = image_date[\"img_date\"].dt.month\n",
    "image_date[\"img_date_week\"]             = image_date[\"img_date\"].dt.week\n",
    "image_date[\"img_date_day\"]              = image_date[\"img_date\"].dt.day\n",
    "image_date[\"img_date_dayofweek\"]        = image_date[\"img_date\"].dt.dayofweek\n",
    "image_date[\"img_date_dayofyear\"]        = image_date[\"img_date\"].dt.dayofyear\n",
    "image_date[\"img_date_hour\"]             = image_date[\"img_date\"].dt.hour\n",
    "image_date[\"img_date_monthBeginMidEnd\"] = image_date[\"img_date_day\"].apply(lambda x: 1 if x<10 else 2 if x<20 else 3)\n",
    "\n",
    "train_df = pd.merge(train_df, image_date, on=\"listing_id\", how=\"left\")\n",
    "test_df = pd.merge(test_df, image_date, on=\"listing_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "megic_features=['img_days_passed','img_date_month','img_date_week','img_date_day','img_date_dayofweek','img_date_dayofyear','img_date_hour','img_date_monthBeginMidEnd']\n",
    "features_to_use.extend(megic_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp = pd.concat([train_df.manager_id,pd.get_dummies(y_train)], axis = 1).groupby('manager_id').mean()\n",
    "temp.columns = ['high_frac','low_frac', 'medium_frac']\n",
    "temp['count'] = train_df.groupby('manager_id').count().iloc[:,1]\n",
    "# compute skill\n",
    "temp['manager_skill'] = temp['high_frac']*2 + temp['medium_frac']\n",
    "\n",
    "# get ixes for unranked managers...\n",
    "unranked_managers_ixes = temp['count']<20\n",
    "# ... and ranked ones\n",
    "ranked_managers_ixes = ~unranked_managers_ixes\n",
    "\n",
    "# compute mean values from ranked managers and assign them to unranked ones\n",
    "mean_values = temp.loc[ranked_managers_ixes, ['high_frac','low_frac', 'medium_frac','manager_skill']].mean()\n",
    "temp.loc[unranked_managers_ixes,['high_frac','low_frac', 'medium_frac','manager_skill']] = mean_values.values\n",
    "\n",
    "train_df = train_df.merge(temp.reset_index(),how='left', left_on='manager_id', right_on='manager_id')\n",
    "\n",
    "test_df = test_df.merge(temp.reset_index(),how='left', left_on='manager_id', right_on='manager_id')\n",
    "\n",
    "features_to_use.extend(['high_frac','low_frac', 'medium_frac','manager_skill'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "savecol=[col for col in train_df.columns if col not in ['Des_TextBlob']]\n",
    "train_df[savecol].to_pickle(\"train.pkl\")\n",
    "savecol.remove('interest_level')\n",
    "test_df[savecol].to_pickle(\"test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features_to_use.extend(['high_frac','low_frac', 'medium_frac','manager_skill'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features_to_use= [col for col in features_to_use if col not in ['high_frac','low_frac', 'medium_frac','manager_skill']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_df['interest_level']='test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_test=pd.concat([train_df,test_df],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "managers_count = train_test['manager_id'].value_counts()\n",
    "\n",
    "train_test['top_10_manager'] = train_test['manager_id'].apply(lambda x: 1 if x in managers_count.index.values[\n",
    "    managers_count.values >= np.percentile(managers_count.values, 90)] else 0)\n",
    "train_test['top_25_manager'] = train_test['manager_id'].apply(lambda x: 1 if x in managers_count.index.values[\n",
    "    managers_count.values >= np.percentile(managers_count.values, 75)] else 0)\n",
    "train_test['top_5_manager'] = train_test['manager_id'].apply(lambda x: 1 if x in managers_count.index.values[\n",
    "    managers_count.values >= np.percentile(managers_count.values, 95)] else 0)\n",
    "train_test['top_50_manager'] = train_test['manager_id'].apply(lambda x: 1 if x in managers_count.index.values[\n",
    "    managers_count.values >= np.percentile(managers_count.values, 50)] else 0)\n",
    "train_test['top_1_manager'] = train_test['manager_id'].apply(lambda x: 1 if x in managers_count.index.values[\n",
    "    managers_count.values >= np.percentile(managers_count.values, 99)] else 0)\n",
    "train_test['top_2_manager'] = train_test['manager_id'].apply(lambda x: 1 if x in managers_count.index.values[\n",
    "    managers_count.values >= np.percentile(managers_count.values, 98)] else 0)\n",
    "train_test['top_15_manager'] = train_test['manager_id'].apply(lambda x: 1 if x in managers_count.index.values[\n",
    "    managers_count.values >= np.percentile(managers_count.values, 85)] else 0)\n",
    "train_test['top_20_manager'] = train_test['manager_id'].apply(lambda x: 1 if x in managers_count.index.values[\n",
    "    managers_count.values >= np.percentile(managers_count.values, 80)] else 0)\n",
    "train_test['top_30_manager'] = train_test['manager_id'].apply(lambda x: 1 if x in managers_count.index.values[\n",
    "    managers_count.values >= np.percentile(managers_count.values, 70)] else 0)\n",
    "\n",
    "buildings_count = train_test['building_id'].value_counts()\n",
    "\n",
    "train_test['top_10_building'] = train_test['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 90)] else 0)\n",
    "train_test['top_25_building'] = train_test['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 75)] else 0)\n",
    "train_test['top_5_building'] = train_test['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 95)] else 0)\n",
    "train_test['top_50_building'] = train_test['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 50)] else 0)\n",
    "train_test['top_1_building'] = train_test['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 99)] else 0)\n",
    "train_test['top_2_building'] = train_test['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 98)] else 0)\n",
    "train_test['top_15_building'] = train_test['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 85)] else 0)\n",
    "train_test['top_20_building'] = train_test['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 80)] else 0)\n",
    "train_test['top_30_building'] = train_test['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 70)] else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_manager_features=['top_10_manager','top_25_manager','top_5_manager','top_50_manager','top_1_manager','top_2_manager','top_15_manager','top_20_manager','top_30_manager']\n",
    "top_buildings_features=['top_10_building','top_25_building','top_5_building','top_50_building','top_1_building','top_2_building','top_15_building','top_20_building','top_30_building']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_to_use.extend(top_manager_features)\n",
    "features_to_use.extend(top_buildings_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df=train_test[train_test['interest_level']!='test']\n",
    "test_df=train_test[train_test['interest_level']=='test']\n",
    "\n",
    "savecol=[col for col in train_df.columns if col not in ['Des_TextBlob']]\n",
    "train_df[savecol].to_pickle(\"train.pkl\")\n",
    "savecol.remove('interest_level')\n",
    "test_df[savecol].to_pickle(\"test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.08457\ttest-mlogloss:1.08492\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 20 rounds.\n",
      "[50]\ttrain-mlogloss:0.727437\ttest-mlogloss:0.741543\n",
      "[100]\ttrain-mlogloss:0.609018\ttest-mlogloss:0.633575\n",
      "[150]\ttrain-mlogloss:0.558608\ttest-mlogloss:0.591715\n",
      "[200]\ttrain-mlogloss:0.528126\ttest-mlogloss:0.56888\n",
      "[250]\ttrain-mlogloss:0.507305\ttest-mlogloss:0.555635\n",
      "[300]\ttrain-mlogloss:0.489207\ttest-mlogloss:0.546051\n",
      "[350]\ttrain-mlogloss:0.47378\ttest-mlogloss:0.538923\n",
      "[400]\ttrain-mlogloss:0.461052\ttest-mlogloss:0.533878\n",
      "[450]\ttrain-mlogloss:0.449469\ttest-mlogloss:0.529993\n",
      "[500]\ttrain-mlogloss:0.439263\ttest-mlogloss:0.52698\n",
      "[550]\ttrain-mlogloss:0.42956\ttest-mlogloss:0.524603\n",
      "[600]\ttrain-mlogloss:0.420904\ttest-mlogloss:0.522613\n",
      "[650]\ttrain-mlogloss:0.412925\ttest-mlogloss:0.521006\n",
      "[700]\ttrain-mlogloss:0.405077\ttest-mlogloss:0.519513\n",
      "[750]\ttrain-mlogloss:0.398104\ttest-mlogloss:0.518395\n",
      "[800]\ttrain-mlogloss:0.391364\ttest-mlogloss:0.517332\n",
      "[850]\ttrain-mlogloss:0.3843\ttest-mlogloss:0.516384\n",
      "[900]\ttrain-mlogloss:0.378166\ttest-mlogloss:0.515956\n",
      "[950]\ttrain-mlogloss:0.371999\ttest-mlogloss:0.515446\n",
      "[1000]\ttrain-mlogloss:0.366179\ttest-mlogloss:0.514782\n",
      "[1050]\ttrain-mlogloss:0.360388\ttest-mlogloss:0.514458\n",
      "[1100]\ttrain-mlogloss:0.354945\ttest-mlogloss:0.514111\n",
      "[1150]\ttrain-mlogloss:0.349361\ttest-mlogloss:0.513901\n",
      "[1200]\ttrain-mlogloss:0.344706\ttest-mlogloss:0.513739\n",
      "[1250]\ttrain-mlogloss:0.339671\ttest-mlogloss:0.513592\n",
      "[1300]\ttrain-mlogloss:0.334541\ttest-mlogloss:0.513438\n",
      "[1350]\ttrain-mlogloss:0.329863\ttest-mlogloss:0.513336\n",
      "Stopping. Best iteration:\n",
      "[1339]\ttrain-mlogloss:0.33078\ttest-mlogloss:0.513246\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_X = sparse.hstack([train_df[features_to_use], tr_sparse]).tocsr()\n",
    "test_X = sparse.hstack([test_df[features_to_use], te_sparse]).tocsr()\n",
    "target_num_map = {'high':0, 'medium':1, 'low':2}\n",
    "train_y = np.array(train_df['interest_level'].apply(lambda x: target_num_map[x]))\n",
    "\n",
    "#preds, model = runlgbm(train_X, train_y)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_X, train_y, test_size=0.20, random_state=42)\n",
    "preds, model = runXGB(X_train, y_train,X_test,y_test,num_rounds=500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds, model = runXGB(train_X, train_y,test_X,num_rounds=1339)\n",
    "#preds=model.predict_proba(test_X)\n",
    "out_df = pd.DataFrame(preds)\n",
    "out_df.columns = [\"high\", \"medium\", \"low\"]\n",
    "out_df[\"listing_id\"] = test_df.listing_id.values\n",
    "out_df.to_csv(\"xgb_startersinglemodel.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "interest_levels = ['low', 'medium', 'high']\n",
    "\n",
    "tau_train = {\n",
    "    'low': 0.694683, \n",
    "    'medium': 0.227529,\n",
    "    'high': 0.077788, \n",
    "}\n",
    "\n",
    "tau_test = {\n",
    "    'low': 0.69195995, \n",
    "    'medium': 0.23108864,\n",
    "    'high': 0.07695141, \n",
    "}\n",
    "\n",
    "def correct(df, train=True, verbose=False):\n",
    "    if train:\n",
    "        tau = tau_train\n",
    "    else:\n",
    "        tau = tau_test\n",
    "        \n",
    "    df_sum = df[interest_levels].sum(axis=1)\n",
    "    df_correct = df[interest_levels].copy()\n",
    "    \n",
    "    if verbose:\n",
    "        y = df_correct.mean()\n",
    "        a = [tau[k] / y[k]  for k in interest_levels]\n",
    "        print( a)\n",
    "    \n",
    "    for c in interest_levels:\n",
    "        df_correct[c] /= df_sum\n",
    "\n",
    "    for i in range(20):\n",
    "        for c in interest_levels:\n",
    "            df_correct[c] *= tau[c] / df_correct[c].mean()\n",
    "\n",
    "        df_sum = df_correct.sum(axis=1)\n",
    "\n",
    "        for c in interest_levels:\n",
    "            df_correct[c] /= df_sum\n",
    "    \n",
    "    if verbose:\n",
    "        y = df_correct.mean()\n",
    "        a = [tau[k] / y[k]  for k in interest_levels]\n",
    "        print( a)\n",
    "\n",
    "    return df_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=correct(out_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"better_prior.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_stack=pd.concat([train_df[['interest_level']+features_to_use],pd.DataFrame(tr_sparse.todense())],axis=1)\n",
    "train_stack.to_csv(\"processed_train.csv\",header=False,index=False)\n",
    "test_stack=pd.concat([test_df[features_to_use],pd.DataFrame(tr_sparse.todense())],axis=1)\n",
    "test_stack.to_csv(\"processed_test.csv\",header=False,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_stack['interest_level']=train_df['interest_level'].apply(lambda x: target_num_map[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_stack.to_csv(\"processed_train.csv\",header=False,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((74659, 271), (49352, 272))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_stack.shape,train_stack.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stackpred=pd.read_csv(\"sigma_stack_pred.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stackpred['listing_id']=test_stack.listing_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stackpred[[\"listing_id\",\"high\", \"medium\", \"low\"]].to_csv(\"preds_stack.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74659, 4)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stackpred.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
