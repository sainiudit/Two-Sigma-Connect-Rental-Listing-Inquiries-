{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'bathrooms', u'bedrooms', u'building_id', u'created', u'description', u'display_address', u'features', u'latitude', u'longitude', u'manager_id', u'photos', u'price']\n",
      "(49352, 74659)\n",
      "('building_id', 11635)\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-90397756baf6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mdf_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'words_of_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0mdf_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'len_of_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m     \u001b[0mdf_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlbl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlbl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/label.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0m_check_numpy_unicode_bug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/numpy/lib/arraysetops.pyc\u001b[0m in \u001b[0;36munique\u001b[0;34m(ar, return_index, return_inverse, return_counts)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \"\"\"\n\u001b[0;32m--> 192\u001b[0;31m     \u001b[0mar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0moptional_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_index\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time; start_time = time.time()\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, preprocessing\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn import pipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "#from bs4 import BeautifulSoup\n",
    "import random; random.seed(7)\n",
    "import xgboost as xgb\n",
    "import datetime as dt\n",
    "\n",
    "train = pd.read_json(open(\"../input/train.json\", \"r\"))\n",
    "y = train.interest_level.values\n",
    "n = len(train)\n",
    "\n",
    "test = pd.read_json(open(\"../input/test.json\", \"r\"))\n",
    "listing_id = test.listing_id.values\n",
    "\n",
    "col = [x for x in train.columns if x not in ['listing_id','interest_level','street_address']]\n",
    "print(col)\n",
    "print(len(train),len(test))\n",
    "\n",
    "def str_stem(s): \n",
    "    if isinstance(s, str):\n",
    "        s = s.lower()\n",
    "        s = s.replace(\"  \",\" \")\n",
    "        #b = BeautifulSoup(s, \"lxml\")\n",
    "        s = b.get_text(\" \").strip()\n",
    "        s = (\" \").join([z for z in s.split(\" \")])\n",
    "        s = (\" \").join([stemmer.stem(z) for z in s.split(\" \")])\n",
    "        s = s.lower().strip()\n",
    "        return s\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "class cust_regression_vals(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    def transform(self, df):\n",
    "        d_col_drops=['xdescription', 'ydescription']\n",
    "        df = df.drop(d_col_drops, axis=1).values\n",
    "        return df\n",
    "\n",
    "class cust_txt_col(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key].apply(str)\n",
    "    \n",
    "df_all = pd.concat((train[col], test[col]), axis=0, ignore_index=True)\n",
    "train = []\n",
    "test = []\n",
    "\n",
    "df_all['photos'] = df_all.photos.apply(len)\n",
    "\n",
    "df_all[\"price_be\"] = df_all[\"price\"]/df_all[\"bedrooms\"]\n",
    "df_all[\"price_ba\"] = df_all[\"price\"]/df_all[\"bathrooms\"]\n",
    "\n",
    "df_all[\"created\"] = pd.to_datetime(df_all[\"created\"])\n",
    "df_all[\"created_year\"] = df_all[\"created\"].dt.year\n",
    "df_all[\"created_month\"] = df_all[\"created\"].dt.month\n",
    "df_all[\"created_day\"] = df_all[\"created\"].dt.day\n",
    "df_all['created_hour'] = df_all[\"created\"].dt.hour\n",
    "df_all['created_weekday'] = df_all['created'].dt.weekday\n",
    "df_all['created_week'] = df_all['created'].dt.week\n",
    "df_all['created_quarter'] = df_all['created'].dt.quarter\n",
    "df_all['created_weekend'] = ((df_all['created_weekday'] == 5) & (df_all['created_weekday'] == 6))\n",
    "df_all['created_wd'] = ((df_all['created_weekday'] != 5) & (df_all['created_weekday'] != 6))\n",
    "df_all['created'] = df_all['created'].map(lambda x: float((x - dt.datetime(1899, 12, 30)).days) + (float((x - dt.datetime(1899, 12, 30)).seconds) / 86400))\n",
    "\n",
    "df_all['x5'] = df_all['latitude'].map(lambda x : round(x,5))\n",
    "df_all['y5'] = df_all['longitude'].map(lambda x : round(x,5))\n",
    "df_all['x4'] = df_all['latitude'].map(lambda x : round(x,4))\n",
    "df_all['y4'] = df_all['longitude'].map(lambda x : round(x,4))\n",
    "df_all['x3'] = df_all['latitude'].map(lambda x : round(x,3))\n",
    "df_all['y3'] = df_all['longitude'].map(lambda x : round(x,3))\n",
    "df_all['x2'] = df_all['latitude'].map(lambda x : round(x,2))\n",
    "df_all['y2'] = df_all['longitude'].map(lambda x : round(x,2))\n",
    "\n",
    "dummies = df_all['features'].str.join(sep=',').str.lower().str.get_dummies(sep=',')\n",
    "df_all = pd.concat([df_all, dummies], axis=1)\n",
    "dummies = []\n",
    "df_all['features'] = df_all.features.apply(len)\n",
    "\n",
    "cat = ['building_id',  'description', 'display_address', 'manager_id']\n",
    "lbl = preprocessing.LabelEncoder()\n",
    "for c in cat:\n",
    "    if c in ['description']:\n",
    "        df_all['x'+c] = df_all[c].map(lambda x:str_stem(x))\n",
    "        df_all['y'+c] = df_all[c].values\n",
    "    df_all['words_of_'+c] = df_all[c].map(lambda x:len(x.strip().split(' ')))\n",
    "    df_all['len_of_'+c] = df_all[c].map(lambda x:len(x.strip()))\n",
    "    df_all[c] = lbl.fit_transform(list(df_all[c].values))\n",
    "    print(c, len(lbl.classes_))\n",
    "\n",
    "train = df_all.iloc[:n]\n",
    "test = df_all.iloc[n:]\n",
    "#df_all = []\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words ='english', max_df=0.9)\n",
    "tsvd = TruncatedSVD(n_components=25, random_state = 7)\n",
    "clf = pipeline.Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "                    transformer_list = [\n",
    "                        ('cst',  cust_regression_vals()),\n",
    "                        ('txt1', pipeline.Pipeline([('s1', cust_txt_col(key='xdescription')), ('tfidf1', tfidf), ('tsvd1', tsvd)])),\n",
    "                        ('txt2', pipeline.Pipeline([('s2', cust_txt_col(key='ydescription')), ('tfidf2', tfidf), ('tsvd2', tsvd)]))\n",
    "                        ],\n",
    "                    transformer_weights = {\n",
    "                        'cst': 1.0,\n",
    "                        'txt1': 1.0,\n",
    "                        'txt2': 1.0\n",
    "                        },\n",
    "                n_jobs = -1\n",
    "                ))])\n",
    "\n",
    "y_val = lbl.fit_transform(y)\n",
    "xtrain = pd.DataFrame(clf.fit_transform(train)).apply(pd.to_numeric)\n",
    "xtrain = xgb.DMatrix(xtrain.values, y_val)\n",
    "xtest = pd.DataFrame(clf.transform(test)).apply(pd.to_numeric)\n",
    "xtest = xgb.DMatrix(xtest.values)\n",
    "\n",
    "param = {}\n",
    "param['objective'] = 'multi:softprob'\n",
    "param['eta'] = 0.1\n",
    "#param['max_depth'] = 4\n",
    "param['silent'] = True\n",
    "param['num_class'] = 3\n",
    "param['eval_metric'] = \"mlogloss\"\n",
    "param['min_child_weight'] = 1\n",
    "param['subsample'] = 0.7\n",
    "param['colsample_bytree'] = 0.7\n",
    "param['seed'] = 7\n",
    "plst = list(param.items())\n",
    "nfolds = 5\n",
    "nrounds = 100\n",
    "\n",
    "model = xgb.cv(plst, xtrain, nrounds, nfolds, early_stopping_rounds=20, verbose_eval=25)\n",
    "best_rounds = np.argmin(model['test-mlogloss-mean'])\n",
    "model = xgb.train(plst, xtrain, best_rounds)\n",
    "print(log_loss(y_val, model.predict(xtrain)))\n",
    "preds = model.predict(xtest)\n",
    "out_df = pd.DataFrame(preds)\n",
    "out_df.columns = lbl.inverse_transform(out_df.columns)\n",
    "out_df[\"listing_id\"] = listing_id\n",
    "out_df.to_csv(\"z09submission01.csv\", index=False)\n",
    "print('Done...',(time.time()-start_time)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
